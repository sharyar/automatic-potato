{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sodapy import Socrata\n",
    "from datetime import datetime\n",
    "import mpu\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.model_selection import cross_val_score "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "We follow a number of steps here that include the following:\n",
    "1. Load our data\n",
    "2. Sample data and reduce the total number of rows used\n",
    "3. Convert datetime fields correctly\n",
    "4. Filter out nulls\n",
    "5. Drop irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CALGARY_PATH = Path(\"data/calgary_2017_2021.parquet\")\n",
    "SOCRATA_API_KEY = \"\" # this should be move to an env var\n",
    "COLUMN_DROP_LIST = [\n",
    "    \"roll_number\",\n",
    "    \"sub_property_use\",\n",
    "    \"point\",\n",
    "    \"land_size_sf\",\n",
    "    \"land_size_ac\",\n",
    "    \"unique_key\",\n",
    "    \"comm_name\",\n",
    "    \"assessment_class_description\",\n",
    "    \"address\",\n",
    "    \"nr_assessed_value\",\n",
    "    \"re_assessed_value\",\n",
    "    \"land_use_designation\",\n",
    "    \"fl_assessed_value\",\n",
    "    \"property_type\"\n",
    "]\n",
    "CALGARY_CENTER_LAT_LONG = (51.047956, -114.068913)\n",
    "DATE_RANGE_SELECTED = (datetime(2017,1,1), datetime(2021,1,1))\n",
    "\n",
    "def load_data() -> pl.DataFrame:\n",
    "    \"\"\"Checks if parquet file exists, if it does, loads that. If not, downloads from \n",
    "    the socrata API. \n",
    "\n",
    "    :return: datafram containing our records\n",
    "    \"\"\"\n",
    "    \n",
    "    if DATA_CALGARY_PATH.exists():\n",
    "        return pl.read_parquet(DATA_CALGARY_PATH)\n",
    "    else:\n",
    "        client = Socrata(\"data.calgary.ca\", SOCRATA_API_KEY)\n",
    "        records = client.get(\"6zp6-pxei\",  limit=3_000_000)\n",
    "        results_df = pl.from_records(records, infer_schema_length=2000)\n",
    "        results_df = results_df.with_columns(pl.col(\"roll_year\").str.strptime(pl.Date, fmt='%Y').cast(pl.Datetime))\n",
    "        results_df.write_parquet(DATA_CALGARY_PATH)\n",
    "        \n",
    "def pre_process_data(df: pl.DataFrame, drop_col_list: list, date_range: tuple[datetime, datetime]) -> pl.DataFrame:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    :param df: _description_\n",
    "    :param drop_col_list: _description_\n",
    "    :param date_range: _description_\n",
    "    :param sample_ratio: ratio of dataset to use\n",
    "    :return: _description_\n",
    "    \"\"\"\n",
    "    # Drop unneeded columns\n",
    "    processed_df = df.drop(drop_col_list)\n",
    "    \n",
    "    # Reduce dataset to specified date range\n",
    "    print(f\"Filtering to set date range\")\n",
    "    len_prior = len(processed_df)\n",
    "    processed_df = processed_df.filter(pl.col(\"roll_year\").is_between(*date_range))\n",
    "    print(f\"Filtering for date complete. Old len: {len_prior}, new_len: {len(processed_df)}\")\n",
    "    \n",
    "    # Filter dataset to residential properties only\n",
    "    processed_df = processed_df.filter(pl.col(\"assessment_class\") == \"RE\")\n",
    "    \n",
    "    # Cast to correct types\n",
    "    processed_df  = processed_df.with_columns(pl.col([\"assessed_value\", \"year_of_construction\"]).cast(pl.Int32))\n",
    "    processed_df = processed_df.with_columns(pl.col(\"land_size_sm\").cast(pl.Float64))\n",
    "    processed_df = processed_df.with_columns(pl.col([\"longitude\", \"latitude\"]).cast(pl.Float64))\n",
    "    \n",
    "    \n",
    "    # Drop nulls\n",
    "    processed_df = processed_df.drop_nulls() # will drop nulls across all columns \n",
    "    \n",
    "    # Drop assessment class column since it is no longer required\n",
    "    processed_df = processed_df.drop([\"assessment_class\"])\n",
    "\n",
    "    return processed_df\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In order to capitalize on our dataset, we do some feature engineering that incldues: \n",
    "\n",
    "1. Calculate distance from city center\n",
    "2. Find and set quadrant for each location \n",
    "3. One hot encode categorical columns\n",
    "   1. Reduce cardinality of columns that have too many categories\n",
    "\n",
    "We suspect that these features may provide some additional information when it comes to modeeling our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Do a few feature engineering things! \n",
    "\n",
    "    :param df: _description_\n",
    "    :return: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    def cal_distance_from_center(lat: float, long: float) -> float:\n",
    "        return mpu.haversine_distance((lat,long), CALGARY_CENTER_LAT_LONG)\n",
    "    \n",
    "    def quadrant(lat: float, long: float) -> str:\n",
    "        NS = \"N\" if lat > CALGARY_CENTER_LAT_LONG[0] else \"S\"\n",
    "        EW = \"E\" if long > CALGARY_CENTER_LAT_LONG[1] else \"W\"\n",
    "        \n",
    "        return NS + EW\n",
    "    \n",
    "    \n",
    "    comm_codes = df.groupby(\"comm_code\").agg([pl.count().alias(\"Count\")]).sort(\"Count\", descending=True)\n",
    "    \n",
    "    def map_community(comm_code):\n",
    "        \"\"\"Used for reducing cardinality in the high cardinality comm_code column! \n",
    "        Only keep values that have higher than 5000 occurences\n",
    "        :param comm_code: _description_\n",
    "        \"\"\"\n",
    "        if comm_code in comm_codes.filter(pl.col(\"Count\") > 5000)[\"comm_code\"].to_list():\n",
    "            return comm_code\n",
    "        else:\n",
    "            return \"Other\"\n",
    "    \n",
    "    # Add columns for city quadrant and distance from center of the city \n",
    "    \n",
    "    processed_df = df.with_columns(\n",
    "        df.select(\n",
    "            pl.struct(\n",
    "                [\"latitude\", \"longitude\"]\n",
    "            ).apply(\n",
    "                lambda x: cal_distance_from_center(x[\"latitude\"], x[\"longitude\"]), pl.Float64\n",
    "            ).alias(\"distance_from_center\")\n",
    "        )\n",
    "    )\n",
    "    processed_df = processed_df.with_columns(\n",
    "        processed_df.select(\n",
    "            pl.struct(\n",
    "                [\"latitude\", \"longitude\"]\n",
    "            ).apply(\n",
    "                lambda x: quadrant(x[\"latitude\"], x[\"longitude\"]), pl.Utf8\n",
    "            ).alias(\"quadrant\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    processed_df = processed_df.with_columns(\n",
    "        processed_df.select(\n",
    "            pl.struct(\n",
    "                [\"comm_code\"]\n",
    "            ).apply(\n",
    "                lambda x: map_community(x[\"comm_code\"]), pl.Utf8\n",
    "            ).alias(\"comm_code\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Convert our date time column into a year column as that is the most useful piece of info there! \n",
    "    processed_df = processed_df.with_columns(\n",
    "        pl.col(\"roll_year\").dt.year().alias(\"roll_year\")\n",
    "    )\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for training testing! \n",
    "\n",
    "At this stage, we need to split the data into a training and test set.\n",
    "We also need to convert data into a pandas format as sklearn pipeline column transformers don't work \n",
    "on polars dataframes.\n",
    "\n",
    "We will also sample our dataset to reduce the training load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preapre_data_for_training(df: pl.DataFrame, sample_frac: float = 0.3) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"We will do some final prep on our data to make sure its ready to go for training! \n",
    "\n",
    "    :param df: _description_\n",
    "    :param sample_frac: _description_\n",
    "    :return: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_df = df.sample(frac=sample_frac, with_replacement=False, seed=42)\n",
    "    \n",
    "    X = processed_df.drop(\"assessed_value\").to_pandas()\n",
    "    y = processed_df[\"assessed_value\"].to_pandas()\n",
    "    \n",
    "    return (X, y)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We build our training pipeline here!\n",
    "\n",
    "1. We first put together the pipeline via bunch of transformers on our columns\n",
    "2. Then we put it all together as a sklearn pipeline \n",
    "3. We then run our training cross validatio to get our results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(X: pd.DataFrame, y:pd.Series) -> np.array:\n",
    "    ct = ColumnTransformer(\n",
    "        [\n",
    "            (\"one_hot_commc_code\", OneHotEncoder(), [\"comm_code\"]),\n",
    "            (\"one_hot_quadrant\", OneHotEncoder(), [\"quadrant\"]),\n",
    "            (\"std_scaler_land_size\", StandardScaler(), [\"land_size_sm\"]),\n",
    "            (\"std_scaler_distance_from_center\", StandardScaler(), [\"distance_from_center\"]),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "\n",
    "    regression_pipeline = Pipeline(\n",
    "        steps=[(\"preprocessor\", ct), (\"regressor\", GradientBoostingRegressor(random_state=42))]\n",
    "    )\n",
    "    scores = cross_val_score(regression_pipeline, X, y, cv=5, scoring=\"neg_root_mean_squared_error\")\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together!\n",
    "\n",
    "1. First we call our data loading function\n",
    "2. We then pass that to our pre-processor\n",
    "3. We then do some final touch ups on the data\n",
    "4. We then run our training\n",
    "5. We then show our training results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering to set date range\n",
      "Filtering for date complete. Old len: 2432429, new_len: 2432429\n"
     ]
    }
   ],
   "source": [
    "# Prep data\n",
    "data = load_data()\n",
    "pre_processed_data = pre_process_data(data, COLUMN_DROP_LIST, DATE_RANGE_SELECTED)\n",
    "engineered_data = feature_engineer(pre_processed_data)\n",
    "X,y = preapre_data_for_training(engineered_data, 0.3)\n",
    "\n",
    "# Run Training\n",
    "scores = run_training(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -848298.43529913  -781377.23648934 -1041393.92058521  -737743.58112529\n",
      "  -924187.35656956]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automatic-potato",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
